### METADATA | ENV ###

# RDBMS is recommended in Skeyecube 4
skeyecube.metadata.url=skeyecube_metadata@jdbc,url=jdbc:mysql://skeyecube-metastore:3306/skeyecube_db,username=root,password=root,maxActive=10,maxIdle=10

# metadata cache sync retry times
skeyecube.metadata.sync-retries=3

# Working folder in HDFS, better be qualified absolute path, make sure user has the right permission to this directory
skeyecube.env.hdfs-working-dir=/user/skeyecube

# DEV|QA|PROD. DEV will turn on some dev features, QA and PROD has no difference in terms of functions.
skeyecube.env=PROD

# skeyecube zk base path
skeyecube.env.zookeeper-base-path=/skeyecube

# Run a TestingServer for curator locally
skeyecube.env.zookeeper-is-local=false

# Connect to a remote zookeeper with the url, should set skeyecube.env.zookeeper-is-local to false
skeyecube.env.zookeeper-connect-string=skeyecube-zookeeper:2181

### SERVER | WEB | RESTCLIENT ###

# Skeyecube server mode, valid value [all, query, job]
skeyecube.server.mode=all

# List of web servers in use, this enables one web server instance to sync up with other servers.
skeyecube.server.cluster-servers=skeyecube-server:7070

# Display timezone on UI,format like[GMT+N or GMT-N]
skeyecube.web.timezone=GMT+2

# Timeout value for the queries submitted through the Web UI, in milliseconds
skeyecube.web.query-timeout=300000

skeyecube.web.cross-domain-enabled=true

#allow user to export query result
skeyecube.web.export-allow-admin=true
skeyecube.web.export-allow-other=false

# Hide measures in measure list of cube designer, separate by comma
skeyecube.web.hide-measures=RAW

#max connections of one route
skeyecube.restclient.connection.default-max-per-route=20

#max connections of one rest-client
skeyecube.restclient.connection.max-total=200

### PUBLIC CONFIG ###
skeyecube.engine.default=6
skeyecube.storage.default=4
skeyecube.web.hive-limit=20
skeyecube.web.contact-mail=
skeyecube.server.external-acl-provider=
skeyecube.source.hive.database-for-flat-table=default

# Default time filter for job list, 0->current day, 1->last one day, 2->last one week, 3->last one year, 4->all
skeyecube.web.default-time-filter=1

### STORAGE ###

# clean real storage after purge operation
# if you want to delete the real storage like htable of deleting segment, you can set it to true
skeyecube.storage.clean-after-delete-operation=false

### JOB ###

# Max job retry on error, default 0: no retry
skeyecube.job.retry=0

# Max count of concurrent jobs running
skeyecube.job.max-concurrent-jobs=10

# The percentage of the sampling, default 100%
skeyecube.job.sampling-percentage=100

# If true, will send email notification on job complete
skeyecube.job.notification-enabled=false
#skeyecube.job.notification-mail-enable-starttls=true
#skeyecube.job.notification-mail-host=smtp.office365.com
#skeyecube.job.notification-mail-port=587
#skeyecube.job.notification-mail-username=skeyecube@example.com
#skeyecube.job.notification-mail-password=mypassword
#skeyecube.job.notification-mail-sender=skeyecube@example.com
skeyecube.job.scheduler.provider.100=com.majesteye.skeye.skeyecube.job.impl.curator.CuratorScheduler
skeyecube.job.scheduler.default=0

### CUBE | DICTIONARY ###

skeyecube.cube.cuboid-scheduler=com.majesteye.skeye.skeyecube.cube.cuboid.DefaultCuboidScheduler
skeyecube.cube.segment-advisor=com.majesteye.skeye.skeyecube.cube.CubeSegmentAdvisor
skeyecube.cube.aggrgroup.max-combination=32768

skeyecube.cube.cubeplanner.enabled=false
skeyecube.cube.cubeplanner.enabled-for-existing-cube=false

# In skeyecube3, the default value of 'skeyecube.cube.cubeplanner.expansion-threshold' is 15.
# Because the storage engine in skeyecube4 changes, the storage size of the same cuboid will become smaller.
# Under the same expansion rate, the number of cuboids in skeyecube4 will be greater than that in skeyecube3.
# In order to keep the pruning result of the phase1 of the cube planner in skeyecube4 roughly the same as that in skeyecube3,
# the default value of this parameter is adjusted to 2.5.
skeyecube.cube.cubeplanner.expansion-threshold=2.5

skeyecube.cube.cubeplanner.recommend-cache-max-size=200
skeyecube.cube.cubeplanner.mandatory-rollup-threshold=1000
skeyecube.cube.cubeplanner.algorithm-threshold-greedy=8
skeyecube.cube.cubeplanner.algorithm-threshold-genetic=23

### QUERY ###

skeyecube.query.cache-enabled=true
skeyecube.query.cache-threshold-scan-count=10240
skeyecube.query.cache-threshold-duration=2000
skeyecube.query.cache-threshold-scan-bytes=1048576
skeyecube.query.large-query-threshold=1000000

# Controls extras properties for Calcite jdbc driver
# all extras properties should undder prefix "skeyecube.query.calcite.extras-props."
# case sensitive, default: true, to enable case insensitive set it to false
# @see org.apache.calcite.config.CalciteConnectionProperty.CASE_SENSITIVE
skeyecube.query.calcite.extras-props.caseSensitive=true
# how to handle unquoted identity, defualt: TO_UPPER, available options: UNCHANGED, TO_UPPER, TO_LOWER
# @see org.apache.calcite.config.CalciteConnectionProperty.UNQUOTED_CASING
skeyecube.query.calcite.extras-props.unquotedCasing=TO_UPPER
# quoting method, default: DOUBLE_QUOTE, available options: DOUBLE_QUOTE, BACK_TICK, BRACKET
# @see org.apache.calcite.config.CalciteConnectionProperty.QUOTING
skeyecube.query.calcite.extras-props.quoting=DOUBLE_QUOTE
# change SqlConformance from DEFAULT to LENIENT to enable group by ordinal
# @see org.apache.calcite.sql.validate.SqlConformance.SqlConformanceEnum
skeyecube.query.calcite.extras-props.conformance=LENIENT

# TABLE ACL
skeyecube.query.security.table-acl-enabled=true

# Usually should not modify this
skeyecube.query.interceptors=com.majesteye.skeye.skeyecube.rest.security.TableInterceptor

skeyecube.query.escape-default-keyword=false

# Usually should not modify this
skeyecube.query.transformers=com.majesteye.skeye.skeyecube.query.util.DefaultQueryTransformer,com.majesteye.skeye.skeyecube.query.util.KeywordDefaultDirtyHack

### SECURITY ###

# Spring security profile, options: testing, ldap, saml
# with "testing" profile, user can use pre-defined name/pwd like SKEYECUBE/ADMIN to login
skeyecube.security.profile=testing

# Admin roles in LDAP, for ldap and saml
skeyecube.security.acl.admin-role=admin

# LDAP authentication configuration
skeyecube.security.ldap.connection-server=ldap://ldap-server:389
skeyecube.security.ldap.connection-username=
skeyecube.security.ldap.connection-password=
# When you use the customized CA certificate library for user authentication based on LDAPs, you need to configure this item.
# The value of this item will be added to the JVM parameter javax.net.ssl.trustStore.
skeyecube.security.ldap.connection-truststore=

# LDAP user account directory;
skeyecube.security.ldap.user-search-base=
skeyecube.security.ldap.user-search-pattern=
skeyecube.security.ldap.user-group-search-base=
skeyecube.security.ldap.user-group-search-filter=(|(member={0})(memberUid={1}))

# LDAP service account directory
skeyecube.security.ldap.service-search-base=
skeyecube.security.ldap.service-search-pattern=
skeyecube.security.ldap.service-group-search-base=

## SAML configurations for SSO
# SAML IDP metadata file location
skeyecube.security.saml.metadata-file=classpath:sso_metadata.xml
skeyecube.security.saml.metadata-entity-base-url=https://hostname/skeyecube
skeyecube.security.saml.keystore-file=classpath:samlKeystore.jks
skeyecube.security.saml.context-scheme=https
skeyecube.security.saml.context-server-name=hostname
skeyecube.security.saml.context-server-port=443
skeyecube.security.saml.context-path=/skeyecube

##################################
### SPARK BUILD ENGINE CONFIGS ###

# Hadoop conf folder, will export this as "HADOOP_CONF_DIR" to run spark-submit
# This must contain site xmls of core, yarn, hive, and hbase in one folder
#skeyecube.env.hadoop-conf-dir=/etc/hadoop/conf

# Switch to spark resources automatic adjustment strategy
skeyecube.spark-conf.auto.prior=true

# Read-Write separation deployment for skeyecube 4
#skeyecube.engine.submit-hadoop-conf-dir=

# log4j properties file for spark
skeyecube.spark.driver.log4j.properties=spark-driver-log4j-default.properties
skeyecube.spark.executor.log4j.properties=spark-executor-log4j-default.properties

# Spark conf (default is in spark/conf/spark-defaults.conf)
skeyecube.engine.spark-conf.spark.master=spark://spark-master:7077
skeyecube.engine.spark-conf.spark.submit.deployMode=client
skeyecube.engine.spark-conf.spark.yarn.queue=default
skeyecube.engine.spark-conf.spark.executor.cores=2
skeyecube.engine.spark-conf.spark.max.executors=2
skeyecube.engine.spark-conf.spark.executor.memory=5G
skeyecube.engine.spark-conf.spark.executor.instances=2
skeyecube.engine.spark-conf.spark.executor.memoryOverhead=2048M
skeyecube.engine.spark-conf.spark.driver.cores=1
skeyecube.engine.spark-conf.spark.driver.memory=4G
skeyecube.engine.spark-conf.spark.driver.memoryOverhead=1G
#skeyecube.engine.spark-conf.spark.shuffle.service.enabled=false
skeyecube.engine.spark-conf.spark.eventLog.enabled=false
#skeyecube.engine.spark-conf.spark.eventLog.dir=hdfs\:///skeyecube/spark-history
#skeyecube.engine.spark-conf.spark.history.fs.logDirectory=hdfs\:///skeyecube/spark-history
skeyecube.engine.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false
skeyecube.engine.spark-conf.spark.executor.extraJavaOptions=-Dfile.encoding=UTF-8 -Dhdp.version=current -Dlog4j.configuration=${skeyecube.spark.executor.log4j.properties} -Dlog4j.debug -Dskeyecube.hdfs.working.dir=${hdfs.working.dir} -Dskeyecube.metadata.identifier=${skeyecube.metadata.url.identifier} -Dskeyecube.spark.category=job -Dskeyecube.spark.project=${job.project} -Dskeyecube.spark.identifier=${job.id} -Dskeyecube.spark.jobName=${job.stepId} -Duser.timezone=${user.timezone}
#skeyecube.engine.spark-conf.spark.sql.shuffle.partitions=1

# manually upload spark-assembly jar to HDFS and then set this property will avoid repeatedly uploading jar at runtime
#skeyecube.engine.spark-conf.spark.yarn.jars=hdfs://localhost:9000/spark2_jars/*
#skeyecube.engine.spark-conf.spark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec

# uncomment for HDP
#skeyecube.engine.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current
#skeyecube.engine.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current
skeyecube.engine.spark-conf.spark.driver.extraJavaOptions=-XX:+CrashOnOutOfMemoryError

##################################
### SPARK QUERY ENGINE CONFIGS ###

# Enlarge cores and memory to improve query performance in production env

#Whether or not to start SparderContext when query server start
skeyecube.query.auto-sparder-context-enabled=false
skeyecube.query.sparder-context.app-name=query-engine

skeyecube.query.spark-conf.spark.master=spark://spark-master:7077
skeyecube.query.spark-conf.spark.driver.cores=1
skeyecube.query.spark-conf.spark.driver.memory=2G
skeyecube.query.spark-conf.spark.driver.memoryOverhead=2G
skeyecube.query.spark-conf.spark.executor.cores=1
skeyecube.query.spark-conf.spark.cores.max=1
skeyecube.query.spark-conf.spark.executor.instances=1
skeyecube.query.spark-conf.spark.executor.memory=4G
skeyecube.query.spark-conf.spark.executor.memoryOverhead=1G
skeyecube.query.spark-conf.spark.serializer=org.apache.spark.serializer.JavaSerializer
#skeyecube.query.spark-conf.spark.sql.shuffle.partitions=40
#skeyecube.query.spark-conf.spark.yarn.jars=hdfs://localhost:9000/spark2_jars/*
skeyecube.query.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false

skeyecube.query.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=${skeyecube.spark.executor.log4j.properties} -Dlog4j.debug \
  -Dskeyecube.hdfs.working.dir=${skeyecube.env.hdfs-working-dir} -Dskeyecube.metadata.identifier=${skeyecube.metadata.url.identifier} -Dskeyecube.spark.category=sparder -Dskeyecube.spark.identifier={{APP_ID}}
# uncomment for HDP
#skeyecube.query.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current
#skeyecube.query.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current

### QUERY PUSH DOWN ###

#skeyecube.query.pushdown.runner-class-name=com.majesteye.skeye.skeyecube.query.pushdown.PushDownRunnerSparkImpl
#skeyecube.query.pushdown.update-enabled=false
